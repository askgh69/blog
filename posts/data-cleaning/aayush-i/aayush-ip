[aayush] -
-It is generally considered a good standard practice to perform the cleaning and augmentation of the data prior to the training of our model. The argument being, once our model is trained, to make it suitable for the ‘new data’ we’ll have to retrain it(tedious). [I am still not clear what do you mean by the description, so it would be better if we communicate regarding this].

{Onto to the GAN part:
GAN stands for Generative Adversarial Network . GAN is made up of two neural networks, known as the generator and the discriminator, where the generator aims to produce data samples that are indistinguishable from real data, while the discriminator strives to differentiate between real and generated data.
The Generator takes the data as input and produces samples mimicking the original data. 
The discriminator is then provided with data samples , including those generated by the Generator, and it classifies them as “real” (comes from the original dataset) or “fake” (comes from the generator).

GANs are widely used for Data Augmentation. A really good GAN will create hyper-realistic and concise samples, which would be a near copy of the original dataset. 

As you might’ve figured out till now, this feature of GAN make them a huge plus for building ML models with a skewed or less extensive dataset. 

Let’s have a look at the Elon and Zuck example itself.
When we are trying to generate more images of these two in the Data Pre-Processing part, we can integrate a GAN model, which’ll do this job effectively for us. 
However, there’s one thing you should be careful of. There is yet to be an intrinsic metric developed which evaluates the quality of generated samples.} - this is a  brief verison of what I plan on writing. It shows the flow of the part,and highlights the points I will include. Lemme know any changes. 
