{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d60a02b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Classifying handwritten digits (THE MNIST!)\"\n",
    "toc: true\n",
    "date: \"2023-09-09\"\n",
    "author: \"Suchit G\"\n",
    "description: \"Understand the fundamental structure of a ML project by building a simple neural network on the MNIST dataset!\"\n",
    "categories:\n",
    "    - Neural Networks\n",
    "    - fast.ai\n",
    "    - ComputerVision\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ffaf6f",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "From MNIST being the first Machine Learning dataset [I heard about](https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=DCyrqxpwpuNR72xJ) to building a model for it, I can surely say Machine Learning is a fascinating subject to study! Building a \"model\" is more than just getting a model and training it. It involves:\n",
    "- Getting the data\n",
    "- Preprocessing the data, i.e., cleaning it, converting it into a format that the model can understand, etc.\n",
    "- Creating the training, validation, and test (test data is not considered in this case) split\n",
    "- Training the model\n",
    "- Fine-tuning hyper-parameters based on the inferences gotten from the accuracy and other metrics over the validation dataset, and then improving the model (not done in this case)\n",
    "\n",
    "Given above is the rough process of building a Machine Learning project. This project maintains a medium level of abstraction and doesn't entirely utilize high-level functions but doesn't go deep into the low-level implementations either. I aim to maintain an understandable and yet not-so-abstracted level of coding throughout. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8efcb6",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "If you are referring to this post as a guide, then you are expected to have the following pre-requisites to fully and deeply understand what's going on:\n",
    "- Basics of Python\n",
    "- Importing modules, methods and calling them in Python\n",
    "- A fundamental idea of how a typical Linear Regression model works (knowledge of simple neural networks recommended but not necessary)\n",
    "- Willingness and the ability to google and read through documentation ;)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c717a5",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "## Some useful resources \n",
    "- [How does a neural net actually work?](https://www.kaggle.com/code/jhoward/how-does-a-neural-net-really-work)\n",
    "- [fastai MNIST chapter](https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb)\n",
    "- [My MNIST (2 digits) notebook](https://github.com/SuchitG04/mnist/blob/main/mnist1.ipynb) (linked again later below)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d40471",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "If I have trouble understanding a piece of code written by others, here's what I do:\n",
    "- Try to speak out loudly and explain the code to myself (rubber ducking).\n",
    "- Search the official docuementation or stack overflow and understand through examples.\n",
    "- If the above two approaches don't work, then ask ChatGPT to explain the code to me. This step works no matter what!\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3816f388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries and modules\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import SubsetRandomSampler, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef477d",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Downloading the Dataset\n",
    "\n",
    "PyTorch's `torchvision` has a module named `datasets` that has some some popular datasets available to download and store it in a directory specified by you. If the dataset already exists, then you just need to give it the path to the dataset and it'll skip downloading it. Click [here](https://pytorch.org/vision/0.8/datasets.html#mnist) to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee692e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: /home/suchitg/mnist/dset/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               RandomRotation(degrees=[-30.0, 30.0], interpolation=nearest, expand=False, fill=0)\n",
      "           )\n",
      "torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "dset = datasets.MNIST(\n",
    "    root='/home/suchitg/mnist/dset/', # Creates a folder named 'dset' if it's not already created\n",
    "    train=True, # Specifies what set of data to download (train or test)\n",
    "    transform=transforms.Compose( # Applies image transformations\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.RandomRotation(degrees=30)\n",
    "        ]\n",
    "    ),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print(dset)\n",
    "print(dset.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a48a10",
   "metadata": {},
   "source": [
    "[`torchvision.transforms.RandomRotation`](https://pytorch.org/vision/0.8/transforms.html#transforms-on-pil-image-and-torch-tensor) is used to [augment](https://suchitg04.github.io/blog/posts/data-cleaning/) the image being loaded into a `dataloader` by randomly rotating the images about 30¬∞. The reason for selecting this transform was because different handwritings write different digits at varying angles. So, the transformation accomodates for this nuance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2f27d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.mnist.MNIST"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f741711",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Train-Val split and Normalization\n",
    "\n",
    "Here, we use [`sklearn.model_selection.train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the data into training and validation datasets. But what is a validation dataset? When we train the model, we improve it by looking at the loss calculated on training set itself (insert reference). But the accuracy (in this case, how many digits the model gets right) is calculated by inputting the images from the validation dataset into the model. That way, we can know how well the model performs on images that it hasn't seen before. If the validation is significantly higher than the training loss (insert reference), then that indicates a case of [overfitting](https://miro.medium.com/v2/resize:fit:1125/1*_7OPgojau8hkiPUiHoGK_w.png).\n",
    "\n",
    "It is also worth noting that, here, we create a stratified train-val split. What that means is that the number of images from each class is roughly equal. This ensures that the model will be equally good in classifying all digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fce5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(dset.data, dset.targets, test_size=0.2, stratify=dset.targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b8956d",
   "metadata": {},
   "source": [
    "Let us now see an example of how a digit looks in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565792fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.image.AxesImage at 0x7f7dc21e7730>, tensor(2))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x.numpy()[99]), train_y[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde7b4e",
   "metadata": {},
   "source": [
    "Each image `train_x[i]` has pixel values between 0 and 255 as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63aa2ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0, dtype=torch.uint8), tensor(255, dtype=torch.uint8))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].min(), train_x[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae2bfaa",
   "metadata": {},
   "source": [
    "Now, we normalize the data. Normalization is not necessarily required here, but we are adding it anyways because it offers a host of benefits. For starters, it helps the model converge, i.e., find a minima of the loss function, faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7ae76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([48000, 784]),\n",
       " torch.Size([12000, 784]),\n",
       " torch.Size([48000, 1]),\n",
       " torch.Size([12000, 1]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.view(-1, 28*28).float() / 255\n",
    "valid_x = valid_x.view(-1, 28*28).float() / 255\n",
    "train_y = train_y.unsqueeze(1)\n",
    "valid_y = valid_y.unsqueeze(1)\n",
    "train_x.shape, valid_x.shape, train_y.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1bb10",
   "metadata": {},
   "source": [
    "Notice how we are unpacking the target variables along the 2nd dimension or as a column vector, so to speak. You'll learn why as you read through further.\n",
    "\n",
    "Let's have a look at an example from our normalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d689875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x[0].min(), train_x[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68dafa5",
   "metadata": {},
   "source": [
    "The pixel values are now between 0 and 1!\n",
    "\n",
    "Look at how preparing and preprocessing the data is as crucial as building a model for it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3423b9",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Loading the Dataset for Training using `DataLoader`\n",
    "\n",
    "Before we load the data into a `DataLoader`, we first have to prepare the data in the proper format for it. `DataLoader`, as per the [documentation](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader) takes map-style and iterable-style datasets. So we supply it with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f90f47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, tuple)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dset = list(zip(train_x, train_y))\n",
    "valid_dset = list(zip(valid_x, valid_y))\n",
    "\n",
    "type(valid_dset), type(valid_dset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83094f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=False)\n",
    "dls = DataLoaders(train_dl, valid_dl) # fast.ai wrapper that encapsulates train_dl and valid_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6414b3e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Creating a Loss Function\n",
    "\n",
    "We now have to create a loss function that suits our dataset. I spent more time figuring out and getting the loss function to work than on the other parts! I'll put both my original code and the optimized version (by ChatGPT). And no, I will not go through the code because well, my brain's already fried from writing it üòÇ. So I suggest you use ChatGPT to explain the code to you or even better, try to figure it out yourselves! It will test your understanding of how the data is structured, and you'll also have to look at what the model spits out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc83b04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mse_loss(preds, targets):\n",
    "#     preds = preds.sigmoid()\n",
    "#     loss = []\n",
    "    \n",
    "#     for pred, target in zip(preds, targets):\n",
    "#         for p in range(len(pred)):\n",
    "#             if p == target:\n",
    "#                 loss.insert(p, (1 - pred[p])**2)\n",
    "#             else:\n",
    "#                 loss.insert(p, pred[p]**2)\n",
    "#             loss[p] = loss[p].mean().view(1)\n",
    "\n",
    "#     loss = torch.cat(loss)\n",
    "#     return loss.mean()\n",
    "\n",
    "# Optimized code by ChatGPT\n",
    "def mse_loss(preds, targets):\n",
    "    preds = preds.sigmoid()\n",
    "    loss = torch.zeros_like(preds)\n",
    "\n",
    "    for i, target in enumerate(targets):\n",
    "        loss[i, target] = (1 - preds[i, target]) ** 2\n",
    "        loss[i] += preds[i] ** 2\n",
    "\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca7bd8",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "I'm using a custom loss function just to demonstrate the performance difference between this loss function (mean squared error) and cross entropy loss (provided by fastai).\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e67227",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Creating a Neural Network and Training it\n",
    "\n",
    "I have chosen the number of neurons and in each layer without any particular reason. So, you can play around with that and see if a lower number works for you as that would bring down the training time. The last layer has to have 10 neurons with output for each digit and that can't be changed.\n",
    "\n",
    "Here, `nn.ReLU` is used to add non-linearity to the model. Otherwise, the model would still be a linear model no matter how many `nn.Linear` you add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "813f3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_net = nn.Sequential(\n",
    "    nn.Linear(28*28, 250),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(250, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf5b285",
   "metadata": {},
   "source": [
    "'tis time Ladies and Gentlemen! 'tis time to train the model! ~~and fret about how slow MSE is or at least how slow my implementation is~~\n",
    "\n",
    "fastai provides a `Learner` class that groups together a model, a loss function and a `DataLoader` object to handle the training. If you want to have a look at the implementation of the entire training process (with minimal to no usage of high-level library functions), then refer to [this](https://github.com/SuchitG04/mnist/blob/main/mnist1.ipynb) implementation on a sample 2 digits MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6a717f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.071596</td>\n",
       "      <td>0.063623</td>\n",
       "      <td>0.857333</td>\n",
       "      <td>01:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.058387</td>\n",
       "      <td>0.057266</td>\n",
       "      <td>0.916000</td>\n",
       "      <td>01:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.055814</td>\n",
       "      <td>0.055529</td>\n",
       "      <td>0.931000</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.054634</td>\n",
       "      <td>0.054546</td>\n",
       "      <td>0.940333</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.053810</td>\n",
       "      <td>0.053983</td>\n",
       "      <td>0.948583</td>\n",
       "      <td>01:16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.053269</td>\n",
       "      <td>0.053475</td>\n",
       "      <td>0.954667</td>\n",
       "      <td>01:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.052900</td>\n",
       "      <td>0.053110</td>\n",
       "      <td>0.957667</td>\n",
       "      <td>01:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.052528</td>\n",
       "      <td>0.052865</td>\n",
       "      <td>0.961500</td>\n",
       "      <td>01:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_mse = Learner(dls, n_net, loss_func=mse_loss, metrics=accuracy)\n",
    "learn_mse.fit(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "732fe10a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.328054</td>\n",
       "      <td>0.274868</td>\n",
       "      <td>0.922333</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.207560</td>\n",
       "      <td>0.196696</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.148225</td>\n",
       "      <td>0.152403</td>\n",
       "      <td>0.956667</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.113479</td>\n",
       "      <td>0.128592</td>\n",
       "      <td>0.962333</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.088385</td>\n",
       "      <td>0.113987</td>\n",
       "      <td>0.967000</td>\n",
       "      <td>00:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.073747</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>0.968583</td>\n",
       "      <td>00:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.061614</td>\n",
       "      <td>0.099214</td>\n",
       "      <td>0.971083</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.048661</td>\n",
       "      <td>0.093742</td>\n",
       "      <td>0.971500</td>\n",
       "      <td>00:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_ce = Learner(dls, n_net, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
    "learn_ce.fit(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d906db8",
   "metadata": {},
   "source": [
    "fastai's `CrossEntropyLossFlat()` is clearly faster than our custom loss function and also seems to be giving slightly higher accuracy.\n",
    "\n",
    "Now, let's use our model to make predictions on different digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f5bde86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first(train_dl)[0][1]), len(first(train_dl)[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "761c4806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.6710e-05, 3.6341e-01, 5.4452e-01, 1.9914e-02, 8.2348e-05, 8.3202e-05,\n",
       "         3.6827e-05, 1.6098e-02, 5.5500e-02, 3.3604e-04],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([2]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_item = first(train_dl) # Gets a batch from the training dataloader\n",
    "torch.softmax(n_net(test_item[0][3]), 0), test_item[1][3] # Note: Do not directly substitute first(dl)[][] in place of test_item.\n",
    "# Because first() gives different batches each time it is run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb363cc",
   "metadata": {},
   "source": [
    "Run the above code cell as many number of times you want because it gives a different digit as input each time. Well! Looks like we have accomplished what we have set out to do, i.e., classify handwritten digits!\n",
    "\n",
    "If you have followed this as a guide, then congrats on reading through everything! To challenge yourself further you can join [one of these](https://www.kaggle.com/competitions?searchQuery=mnist) Kaggle competitions and make a submission. But note that your \"data pipeline\" (yes, I feel like a God using this word) for the competition has to be different than what we have done here because you'll be given a CSV file to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bf0222",
   "metadata": {},
   "source": [
    "Thank you for reading my blog. You can reach out to me through my socials here:\n",
    "\n",
    "- Discord - ‚Äúlostsquid.‚Äù\n",
    "- LinkedIn - /in/suchitg04/\n",
    "\n",
    "I hope to see you soon. Until then üëã"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
